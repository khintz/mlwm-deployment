# Default to Python 3.12 image with PyTorch Lightning and CUDA support
ARG BASE_IMAGE="pytorchlightning/pytorch_lightning:base-cuda-py3.12-torch2.6-cuda12.4.1"
FROM ${BASE_IMAGE}

WORKDIR /workspace

COPY pyproject.toml .
COPY *.yaml ./
COPY entry.sh ./
COPY src/ ./src

# Download inference artifact from S3
ARG DEFAULT_ARTIFACT="s3://mlwm-artifacts/inference-artifacts/surface-dummy-model.zip"
ENV MLWM_INFERENCE_ARTIFACT=${DEFAULT_ARTIFACT}

# Install awscli
RUN apt-get update && apt-get install -y python3 python3-pip unzip
RUN pip3 install awscli


ARG AWS_ACCESS_KEY_ID
ARG AWS_SECRET_ACCESS_KEY
ARG AWS_DEFAULT_REGION
ENV AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
ENV AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
ENV AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION

# Get inference artifact from S3
RUN aws s3 cp $MLWM_INFERENCE_ARTIFACT ./inference_artifact.zip
RUN mkdir -p /workspace/inference_artifact
RUN unzip inference_artifact.zip -d /workspace/inference_artifact
RUN rm inference_artifact.zip
# List files in the inference artifact directory for verification
RUN ls -la /workspace/inference_artifact

# Install uv
RUN curl -LsSf https://astral.sh/uv/install.sh | sh

ENV PATH="/root/.local/bin:$PATH"

# Install directly into system python
# https://github.com/astral-sh/uv/issues/8085#issuecomment-2707744397
# RUN uv export | uv pip install --system -r -
# RUN uv export | uv pip install --system --break-system-packages -r -
# RUN uv venv create -p 3.11 --system-site-packages --clear
# RUN uv sync

# export current torch version to constraints file
RUN python -c 'import torch; print(f"torch=={torch.__version__}")' > constraints.txt
# install with constraints
RUN uv pip install --break-system-packages --system --constraints constraints.txt .

ENTRYPOINT ["/bin/bash"]
# Set the default command to run when the container starts
CMD ["entry.sh"]
